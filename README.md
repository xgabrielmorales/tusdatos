# TusDatos: Prueba T칠cnica 游뱁

Este repositorio contiene la resoluci칩n a la prueba t칠cnica para el cargo de Backend Developer de la empresa [Tusdatos.co](https://www.tusdatos.co/) donde se solicit칩 extraer la informaci칩n del sitio [Consulta de Procesos Judiciales de Ecuador](https://procesosjudiciales.funcionjudicial.gob.ec/expel-busqueda-avanzada) usando t칠cnicas de scraping con Python.

## Estrateg칤a

La estrategia utilizada para la resoluci칩n de la prueba t칠cnica fue hacer uso de API Scraping, dado que el sitio web propuesto expon칤a una REST API, lo cual hac칤a m치s eficiente, computacionalmente hablando, la extracci칩n de los datos en comparaci칩n con hacer Web Scraping.

Para la primera parte de la prueba, se implement칩 una funcionalidad as칤ncrona que consume y recupera la informaci칩n del sitio para posteriormente ser almacenada en una base de datos no relacional (MongoDB). Esta funcionalidad permite la extracci칩n de los **casos**, **detalles** y **acciones judiciales** de todos los procedimientos judiciales del actor o demandado.

Para la segunda parte, se implement칩 una API REST (en FastAPI) que expone los datos extra칤dos del sitio web; esta informaci칩n est치 protegida y solo es accesible para los usuarios autenticados. Adicionalmente, esta API permite la consulta o actualizaci칩n de los datos de un actor o demandado para posteriormente ser consultados.

## Inicializa el proyecto

1. Clona este repositorio en tu m치quina local:
   ```bash
   $ git clone https://github.com/xgabrielmorales/tusdatos.git
   ````

2. Dentro de la carpeta del proyecto, crea un archivo .env y adicional las variables de entorno:
   ```bash
   $ cd tusdatos
   ````
   ```bash
   $ touch .env
   ````
   Aqu칤 tienes unas de ejemplo que pueden servir: 游뱇
    ```bash
    # Base
    # ==============================================================================
    SECRET_KEY=3RWM3zT68QEaOacQiYmSVzNyOHnJMpqVQi8mS2zN
    
    # Mongo DB
    # ==============================================================================
    MONGO_HOST=tusdatos-mongo-db
    MONGO_INITDB_ROOT_PASSWORD=password
    MONGO_INITDB_ROOT_USERNAME=admin
    
    # Postgres DB
    # ==============================================================================
    POSTGRES_DB=tusdatos-db
    POSTGRES_PASSWORD=e8617cc1dccfbeefe3777f6816bd6cce
    POSTGRES_HOST=tusdatos-postgres-db
    POSTGRES_USER=tusdatos-db
    ```


3. Haz build de la imagen de docker con el siguiente comando:
   ```bash
   $ docker compose -f docker-compose.dev.yml build
   ```

4. Levanta el servicio de la base de datos y aplica las migraciones:
   ```bash
   $ docker compose -f docker-compose.dev.yml up -d tusdatos-postgres-db
   ```
   ```bash
   $ docker compose -f docker-compose.dev.yml run --rm tusdatos alembic upgrade head
   ```

5. Levanta todos los servicios restantes:
   ```bash
   $ docker compose -f docker-compose.dev.yml up -d
   ```

Excelente, ya est치s listo para consumir el REST API. 游눩

## Documentaci칩n del API

Si completaste las secci칩n aterior, entonces est치s listo para consumir el API, la documentaci칩n de esta puede ser accedida desde tu navegador el la siguiente URL: http://localhost:8000/docs

> [!NOTE]
> Si no te es posible ver el sitio web de la documentaci칩n de OpenAPI de FastAPI es porque algo sali칩 mal en la secci칩n anterior. Asegurate de que los servicios de docker del proyecto est칠n corriendo y que no exista ning칰n otro servicios ocupando ya el puerto `8000`.
> 

## Consultas ~paralelas~ concurrentes

Ejecutar 15 consultas paralelas implica un requerimiento de hardware, donde 15 procesos est치n siendo ejecutados simult치neamente por distintos CPUs. Lamentablemente, mi m치quina no tiene tantos CPUs, por lo que no podr칤a replicar con exactitud el escenario. Sin embargo, el escenario que mi m치quina s칤 permite replicar es el de 15 consultas concurrentes, es decir, 15 procesos que ser치n ejecutados en un mismo periodo de tiempo (pero no simult치neos).

### Preparaci칩n

Antes de iniciar los distintos escenarios ser치 necesario preparar un punto de entrada para nuestra clase scraper, que nos servir치 para m치s adelante:

```python
# run.py

import asyncio

from tusdatos.services.scraper import JudicialProcessScraper


async def main():
    user_document_num = "0968599XXXXXX"
    search_role = "ACTOR"

    scraper = JudicialProcessScraper(
        user_document_num=user_document_num,
        search_role=search_role,
    )
    await scraper.extract_data()


if __name__ == "__main__":
    asyncio.run(main())
```

### x1 Consultas Concurrentes

El primer escenario ser치 ejecutar una 칰nica consulta concurrente; para ello, bastar치 con ejecutar el script anterior, observar el tiempo tardado y el comportamiento del scraper.

![](https://i.ibb.co/MN3yQFM/x1-consulta-concurrente.png)

Como vemos, es relativamente r치pido para la cantidad de consultas que realiza y no present칩 ning칰n inconveniente. Veamos qu칠 pasa si aumentamos a칰n m치s el n칰mero de consultas concurrentes.

### x5 Consultas Concurrentes

Para este escenario, ser치 necesario hacer una peque침a modificaci칩n en nuestro punto de entrada, permitiendo as칤 un grupo de 5 corutinas, ejecutarlas y validar el comportamiento:

```python
# run.py

import asyncio

from tusdatos.services.scraper import JudicialProcessScraper


async def main():
    user_document_num = "0968599XXXXXX"
    search_role = "ACTOR"

    corroutines = []
    for _ in range(0, 5):
        scraper = JudicialProcessScraper(
            user_document_num=user_document_num,
            search_role=search_role,
        )
        corroutines.append(scraper.extract_data())

    await asyncio.gather(*corroutines)


if __name__ == "__main__":
    asyncio.run(main())
```

Ahora hagamos la prueba y veamos como se comporta:

![](https://i.ibb.co/1LcPRSC/x5-consulta-concurrente.png)

Como se evidencia no hay ning칰n problema en correr 5 consultas concurrentes y el incremento de tiempo es aceptable.
   
### x10 Consultas Concurrentes

Aumentemos la apuesta, veamos que ocurre con 10 consultas concurrentes:

```python
# run.py

# Mismo c칩digo anterior...

    corroutines = []
    for _ in range(0, 10): # <-- Incremento
        scraper = JudicialProcessScraper(
            user_document_num=user_document_num,
            search_role=search_role,
        )
        corroutines.append(scraper.extract_data())

# Contin칰a el c칩digo...
```

### x25 Consultas Concurrentes

Qu칠 pasar칤a si se hacen 25 consultas concurrentes a la p치gina de consulta. Veamoslo:

![image](https://github.com/xgabrielmorales/tusdatos/assets/50029987/96326c09-4a30-4bcf-932c-f39537f0eeb5)

Como vemos, lo 칰nico que ocurre es aumentar el tiempo de respuesta, sin embargo, el servidor de consulta responde sin ning칰n problema.

### Problemas de conexi칩n y l칤mite de requests

El caso de ejemplo anterior se realiz칩 en un Droplet de DigitalOcean y la decisi칩n no fue arbitraria. Durante el desarrollo de esta prueba, al hacer pruebas en mi m치quina local, obtuve tiempos de respuesta similares a los expuestos en los ejemplos anteriores. Sin embargo, en los 칰ltimos d칤as de la prueba, empec칠 a tener problemas de conexi칩n con el servidor de consultas:

![](https://i.ibb.co/bFPh57S/httpx-error-conexion.png)

De alguna manera, desde mi m치quina local, las conexiones fallaban cada vez con m치s frecuencia. La soluci칩n a este problema fue agregar un sem치foro con la intenci칩n de limitar el n칰mero de corutinas relacionadas con consultas HTTP al servidor de consultas:

https://github.com/xgabrielmorales/tusdatos/blob/85d621e9ee517daa452e9a590d09f22024fb4be7/tusdatos/services/scraper.py#L27
https://github.com/xgabrielmorales/tusdatos/blob/85d621e9ee517daa452e9a590d09f22024fb4be7/tusdatos/services/scraper.py#L130

Esto supuso un alivio considerable, pero, por supuesto, los tiempos de respuesta iban a aumentar. Sin embargo, la implementaci칩n del sem치foro se conserv칩, dado que no es recomendable abusar del servidor de consultas debido a la cantidad de solicitudes que se realizan por cada consulta 칰nica (aproximadamente +150 solicitudes por consulta para los n칰meros de documento compartidos).
